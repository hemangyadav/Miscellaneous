---
title: "PracticalML-Homework"
output: html_document
date: "2023-11-27"
author: "Hemang Yadav"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(caret)
library(caretEnsemble)
library(kernlab)
library(tidyverse)
library(ISLR)
library(Hmisc)
library(gridExtra)
library(grid)
library(RANN)
library(mice)
library(randomForest)
library(skimr)

```

## Assignment Instructions

Instructions: The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

Your submission for the Peer Review portion should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).

## Data Wrangling
First we will preprocess data, impute missing data using multiple imputation and get the training set ready for model building

```{r wrangle}

training <- read_csv('pml-training.csv'); testing <- read_csv('pml-testing.csv')
dim(training); dim(testing)

skim(training)
training <- training[, -c(1,2,3,4,5,6,7)] # Drop identifiers
# Drop the predictors which have high rates of missing data (can see using skim)
training <- training[ , colSums(is.na(training)) == 0]

training[rowSums(is.na(training)) > 0,] # Visualize NAs - should be zero
training$classe <- as.factor(training$classe)

glimpse(training)



```

## Model developemnt

Now we will use caretEnsemble to evaluate a range of different models. The goal is to use the training dataset variables to predict classe. 

We will use 10-fold cross validation, evaluating a range of models that include tree based models (e.g. random forest, CART) as well as GBM and penalized regression (glmnet). 

To help choose the final model we will visualize the accuracy of the different models tested using the resamples function. 

```{r modeldev}
predictors <- names(training)[-c(53)]
formula <- as.formula(paste("classe ~", paste(predictors, collapse = " + "))) # Create a formula for the outcome and predictors

ctrl <- trainControl(method = "repeatedcv",
                     number = 10, 
                     index = createFolds(training$classe, 5),
                     savePredictions="final",
                     verbose = FALSE)

algorithmList <- c("gbm", "rf", "glmnet", "rpart")

models <- caretList(formula, 
                    data = training, 
                    trControl = ctrl, 
                    methodList = algorithmList)
results <- resamples(models)
summary(results)
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)


```

## Model selection

Reviewing the above models, the performance of RF seems best. Lets look at out of sample prediction expectation using the RF model. Since we saved predictions, we can access this information.  

```{r modelsel}

model.rf <- train(formula, data = training, method = "rf", trControl = ctrl)

model.rf$finalModel

```

With this we can see the out of bag estimate of the error rate is 0.4%. 

## Predict test set

Now we have our final model we can generate predictions using the test data. 

```{r predict, echo=FALSE}

predictions <- predict(model.rf,newdata=testing)

print(predictions)

```